{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFszuozM4N_8",
    "outputId": "65f6894e-f7b4-4338-9f88-2a49f4f8eff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
      "Collecting dgl-cu116\n",
      "  Downloading https://data.dgl.ai/wheels/dgl_cu116-0.9.1.post1-cp39-cp39-manylinux1_x86_64.whl (246.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 246.3 MB 8.1 kB/s eta 0:00:0101 |▊                               | 5.5 MB 846 kB/s eta 0:04:45\n",
      "\u001b[?25hCollecting dglgo\n",
      "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dgl-cu116) (1.23.4)\n",
      "Collecting psutil>=5.8.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dgl-cu116) (2.28.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.1\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 56.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 33.8 MB 93.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rdkit-pypi\n",
      "  Downloading rdkit_pypi-2022.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.3 MB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 81.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ogb>=1.3.3\n",
      "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting autopep8>=1.6.0\n",
      "  Downloading autopep8-2.0.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typer>=0.4.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting ruamel.yaml>=0.17.20\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 107.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic>=1.9.0\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 88.0 MB/s eta 0:00:01    |███                             | 1.3 MB 88.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting isort>=5.10.1\n",
      "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 97.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpydoc>=1.1.0\n",
      "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 84.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycodestyle>=2.9.1\n",
      "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 710 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tomli\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
      "Collecting sphinx>=4.2\n",
      "  Downloading sphinx-5.3.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 94.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.1)\n",
      "Collecting pandas>=0.24.0\n",
      "  Downloading pandas-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 104.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ogb>=1.3.3->dglgo) (1.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ogb>=1.3.3->dglgo) (1.26.12)\n",
      "Collecting outdated>=0.2.0\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "Requirement already satisfied: setuptools>=44 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[K     |████████████████████████████████| 498 kB 104.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic>=1.9.0->dglgo) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->dgl-cu116) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->dgl-cu116) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->dgl-cu116) (3.4)\n",
      "Collecting ruamel.yaml.clib>=0.2.6\n",
      "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
      "\u001b[K     |████████████████████████████████| 519 kB 93.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 98.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 23.6 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting snowballstemmer>=2.0\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 2.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 110.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 9.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.20,>=0.14\n",
      "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "\u001b[K     |████████████████████████████████| 570 kB 82.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 22.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (4.11.4)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting babel>=2.9\n",
      "  Downloading Babel-2.11.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 115.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imagesize>=1.3\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pygments>=2.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.13.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (21.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-metadata>=4.8->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=21.0->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->ogb>=1.3.3->dglgo) (0.37.1)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from rdkit-pypi->dglgo) (9.3.0)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=1e2cc025c60e80ac354d9148c156b9ce0be05f9aebfceb38be7cb13655327404\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
      "Successfully built littleutils\n",
      "Installing collected packages: pytz, threadpoolctl, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, scipy, littleutils, joblib, imagesize, docutils, babel, alabaster, tqdm, tomli, sphinx, scikit-learn, ruamel.yaml.clib, pycodestyle, pandas, outdated, click, typer, ruamel.yaml, rdkit-pypi, PyYAML, pydantic, psutil, ogb, numpydoc, networkx, isort, autopep8, dglgo, dgl-cu116\n",
      "Successfully installed PyYAML-6.0 alabaster-0.7.12 autopep8-2.0.0 babel-2.11.0 click-8.1.3 dgl-cu116-0.9.1.post1 dglgo-0.0.2 docutils-0.19 imagesize-1.4.1 isort-5.10.1 joblib-1.2.0 littleutils-0.2.2 networkx-2.8.8 numpydoc-1.5.0 ogb-1.3.5 outdated-0.2.2 pandas-1.5.2 psutil-5.9.4 pycodestyle-2.10.0 pydantic-1.10.2 pytz-2022.6 rdkit-pypi-2022.9.2 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 scikit-learn-1.2.0 scipy-1.9.3 snowballstemmer-2.2.0 sphinx-5.3.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 threadpoolctl-3.1.0 tomli-2.0.1 tqdm-4.64.1 typer-0.7.0\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install dgl-cu116 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
    "!pip install emoji\n",
    "!pip install wordsegment\n",
    "!pip install ekphrasis\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wI4we4EB4Kl5",
    "outputId": "49ce124f-aaf6-438f-c98f-e38237dedcf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /home/studio-lab-user/.ekphrasis/stats/twitter/counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /home/studio-lab-user/.ekphrasis/stats/twitter/counts_2grams.txt\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
      "Reading twitter - 1grams ...\n",
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
      "torch.cuda.is_available() True\n",
      "device is cuda\n",
      "Downloading: 100%|██████████████████████████████| 481/481 [00:00<00:00, 425kB/s]\n",
      "Downloading: 100%|███████████████████████████| 501M/501M [00:05<00:00, 88.9MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|███████████████████████████| 899k/899k [00:00<00:00, 10.7MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 7.28MB/s]\n",
      "Epoch 0\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [04:54<00:00,  1.05s/it]\n",
      "process time: 294.57755517959595\n",
      "loss = 6.3453\n",
      "Macro-accuracy = 0.8313\n",
      "Macro-recall = 0.8313\n",
      "Macro- precision = 0.8313\n",
      "Macro-F1 = 0.8313\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:45<00:00,  2.65it/s]\n",
      "loss = 4.7943\n",
      "Macro-accuracy = 0.7908\n",
      "Macro-recall = 0.8759\n",
      "Macro- precision = 0.6396\n",
      "Macro-F1 = 0.6546\n",
      "AUC = 0.8956\n",
      "Best epoch: [(0, '0.6546')]\n",
      "f1: [0.6546]\n",
      "accuracy: [0.7908]\n",
      "Best test f1: 0.6546\n",
      "Best test accuracy: 0.7908\n",
      "Best test recall: 0.8759\n",
      "Best test precision: 0.6396\n",
      "Best AUC: 0.8956\n",
      "====================\n",
      "Epoch 1\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:05<00:00,  1.09s/it]\n",
      "process time: 299.8393362760544\n",
      "loss = 0.7222\n",
      "Macro-accuracy = 0.9367\n",
      "Macro-recall = 0.9368\n",
      "Macro- precision = 0.9368\n",
      "Macro-F1 = 0.9367\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:41<00:00,  2.91it/s]\n",
      "loss = 1.2347\n",
      "Macro-accuracy = 0.9199\n",
      "Macro-recall = 0.9150\n",
      "Macro- precision = 0.7508\n",
      "Macro-F1 = 0.8043\n",
      "AUC = 0.9470\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043')]\n",
      "f1: [0.6546, 0.8043]\n",
      "accuracy: [0.7908, 0.9199]\n",
      "Best test f1: 0.8043\n",
      "Best test accuracy: 0.9199\n",
      "Best test recall: 0.9150\n",
      "Best test precision: 0.7508\n",
      "Best AUC: 0.9470\n",
      "====================\n",
      "Epoch 2\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:13<00:00,  1.12s/it]\n",
      "process time: 304.3730394045512\n",
      "loss = 0.2178\n",
      "Macro-accuracy = 0.9589\n",
      "Macro-recall = 0.9589\n",
      "Macro- precision = 0.9589\n",
      "Macro-F1 = 0.9589\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:45<00:00,  2.63it/s]\n",
      "loss = 0.2573\n",
      "Macro-accuracy = 0.9358\n",
      "Macro-recall = 0.9023\n",
      "Macro- precision = 0.7817\n",
      "Macro-F1 = 0.8276\n",
      "AUC = 0.9648\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276')]\n",
      "f1: [0.6546, 0.8043, 0.8276]\n",
      "accuracy: [0.7908, 0.9199, 0.9358]\n",
      "Best test f1: 0.8276\n",
      "Best test accuracy: 0.9358\n",
      "Best test recall: 0.9023\n",
      "Best test precision: 0.7817\n",
      "Best AUC: 0.9648\n",
      "====================\n",
      "Epoch 3\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:24<00:00,  1.16s/it]\n",
      "process time: 309.3831961154938\n",
      "loss = 0.0588\n",
      "Macro-accuracy = 0.9744\n",
      "Macro-recall = 0.9744\n",
      "Macro- precision = 0.9744\n",
      "Macro-F1 = 0.9744\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:45<00:00,  2.65it/s]\n",
      "loss = 0.3013\n",
      "Macro-accuracy = 0.9463\n",
      "Macro-recall = 0.8880\n",
      "Macro- precision = 0.8106\n",
      "Macro-F1 = 0.8437\n",
      "AUC = 0.9608\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463]\n",
      "Best test f1: 0.8437\n",
      "Best test accuracy: 0.9463\n",
      "Best test recall: 0.8880\n",
      "Best test precision: 0.8106\n",
      "Best AUC: 0.9608\n",
      "====================\n",
      "Epoch 4\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:19<00:00,  1.14s/it]\n",
      "process time: 311.4015537261963\n",
      "loss = 0.0259\n",
      "Macro-accuracy = 0.9794\n",
      "Macro-recall = 0.9795\n",
      "Macro- precision = 0.9794\n",
      "Macro-F1 = 0.9794\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:41<00:00,  2.92it/s]\n",
      "loss = 0.1407\n",
      "Macro-accuracy = 0.9525\n",
      "Macro-recall = 0.8801\n",
      "Macro- precision = 0.8333\n",
      "Macro-F1 = 0.8547\n",
      "AUC = 0.9690\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437'), (4, '0.8547')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437, 0.8547]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463, 0.9525]\n",
      "Best test f1: 0.8547\n",
      "Best test accuracy: 0.9525\n",
      "Best test recall: 0.8801\n",
      "Best test precision: 0.8333\n",
      "Best AUC: 0.9690\n",
      "====================\n",
      "Epoch 5\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:12<00:00,  1.11s/it]\n",
      "process time: 311.5307201941808\n",
      "loss = 0.0135\n",
      "Macro-accuracy = 0.9826\n",
      "Macro-recall = 0.9825\n",
      "Macro- precision = 0.9826\n",
      "Macro-F1 = 0.9826\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:46<00:00,  2.60it/s]\n",
      "loss = 0.2445\n",
      "Macro-accuracy = 0.9504\n",
      "Macro-recall = 0.8846\n",
      "Macro- precision = 0.8248\n",
      "Macro-F1 = 0.8514\n",
      "AUC = 0.9636\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437'), (4, '0.8547')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437, 0.8547, 0.8514]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463, 0.9525, 0.9504]\n",
      "Best test f1: 0.8547\n",
      "Best test accuracy: 0.9525\n",
      "Best test recall: 0.8801\n",
      "Best test precision: 0.8333\n",
      "Best AUC: 0.9690\n",
      "====================\n",
      "Epoch 6\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:25<00:00,  1.16s/it]\n",
      "process time: 313.52321696281433\n",
      "loss = 0.0033\n",
      "Macro-accuracy = 0.9902\n",
      "Macro-recall = 0.9902\n",
      "Macro- precision = 0.9902\n",
      "Macro-F1 = 0.9902\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:43<00:00,  2.73it/s]\n",
      "loss = 0.1501\n",
      "Macro-accuracy = 0.9465\n",
      "Macro-recall = 0.8654\n",
      "Macro- precision = 0.8151\n",
      "Macro-F1 = 0.8378\n",
      "AUC = 0.9675\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437'), (4, '0.8547')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437, 0.8547, 0.8514, 0.8378]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463, 0.9525, 0.9504, 0.9465]\n",
      "Best test f1: 0.8547\n",
      "Best test accuracy: 0.9525\n",
      "Best test recall: 0.8801\n",
      "Best test precision: 0.8333\n",
      "Best AUC: 0.9690\n",
      "====================\n",
      "Epoch 7\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:00<00:00,  1.07s/it]\n",
      "process time: 311.88723626732826\n",
      "loss = 0.0030\n",
      "Macro-accuracy = 0.9885\n",
      "Macro-recall = 0.9885\n",
      "Macro- precision = 0.9885\n",
      "Macro-F1 = 0.9885\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:42<00:00,  2.85it/s]\n",
      "loss = 0.2059\n",
      "Macro-accuracy = 0.9559\n",
      "Macro-recall = 0.8306\n",
      "Macro- precision = 0.8677\n",
      "Macro-F1 = 0.8479\n",
      "AUC = 0.9659\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437'), (4, '0.8547')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437, 0.8547, 0.8514, 0.8378, 0.8479]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463, 0.9525, 0.9504, 0.9465, 0.9559]\n",
      "Best test f1: 0.8547\n",
      "Best test accuracy: 0.9525\n",
      "Best test recall: 0.8801\n",
      "Best test precision: 0.8333\n",
      "Best AUC: 0.9690\n",
      "====================\n",
      "Epoch 8\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [05:01<00:00,  1.08s/it]\n",
      "process time: 310.7091420491536\n",
      "loss = 0.0023\n",
      "Macro-accuracy = 0.9907\n",
      "Macro-recall = 0.9907\n",
      "Macro- precision = 0.9907\n",
      "Macro-F1 = 0.9907\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:45<00:00,  2.62it/s]\n",
      "loss = 0.1301\n",
      "Macro-accuracy = 0.9473\n",
      "Macro-recall = 0.9000\n",
      "Macro- precision = 0.8122\n",
      "Macro-F1 = 0.8489\n",
      "AUC = 0.9687\n",
      "Best epoch: [(0, '0.6546'), (1, '0.8043'), (2, '0.8276'), (3, '0.8437'), (4, '0.8547')]\n",
      "f1: [0.6546, 0.8043, 0.8276, 0.8437, 0.8547, 0.8514, 0.8378, 0.8479, 0.8489]\n",
      "accuracy: [0.7908, 0.9199, 0.9358, 0.9463, 0.9525, 0.9504, 0.9465, 0.9559, 0.9473]\n",
      "Best test f1: 0.8547\n",
      "Best test accuracy: 0.9525\n",
      "Best test recall: 0.8801\n",
      "Best test precision: 0.8333\n",
      "Best AUC: 0.9690\n",
      "====================\n",
      "============early stopping==============\n",
      "now: 8    last: 4\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "!python train_joint.py \\\n",
    "  -bs=32 \\\n",
    "  -lr_other=5e-5 \\\n",
    "  -lr_gat=1e-2 \\\n",
    "  -ep=20 \\\n",
    "  -dr=0.5 \\\n",
    "  -ad=0.1 \\\n",
    "  -hs=768 \\\n",
    "  --model=roberta \\\n",
    "  --clip \\\n",
    "  --cuda=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
      "Reading twitter - 1grams ...\n",
      "torch.cuda.is_available() True\n",
      "device is cuda\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 522kB/s]\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:04<00:00, 95.4MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 6.06MB/s]\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 24.6kB/s]\n",
      "Epoch 0\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:33<00:00,  1.83it/s]\n",
      "process time: 153.35329246520996\n",
      "loss = 2.1497\n",
      "Macro-accuracy = 0.8710\n",
      "Macro-recall = 0.8710\n",
      "Macro- precision = 0.8710\n",
      "Macro-F1 = 0.8710\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:24<00:00,  4.95it/s]\n",
      "loss = 0.5417\n",
      "Macro-accuracy = 0.9301\n",
      "Macro-recall = 0.9063\n",
      "Macro- precision = 0.7692\n",
      "Macro-F1 = 0.8186\n",
      "AUC = 0.9622\n",
      "Best epoch: [(0, '0.8186')]\n",
      "f1: [0.8186]\n",
      "accuracy: [0.9301]\n",
      "Best test f1: 0.8186\n",
      "Best test accuracy: 0.9301\n",
      "Best test recall: 0.9063\n",
      "Best test precision: 0.7692\n",
      "Best AUC: 0.9622\n",
      "====================\n",
      "Epoch 1\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:40<00:00,  1.75it/s]\n",
      "process time: 156.88357532024384\n",
      "loss = 0.2067\n",
      "Macro-accuracy = 0.9573\n",
      "Macro-recall = 0.9573\n",
      "Macro- precision = 0.9573\n",
      "Macro-F1 = 0.9573\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:24<00:00,  4.85it/s]\n",
      "loss = 0.9712\n",
      "Macro-accuracy = 0.8920\n",
      "Macro-recall = 0.9183\n",
      "Macro- precision = 0.7136\n",
      "Macro-F1 = 0.7659\n",
      "AUC = 0.9522\n",
      "Best epoch: [(0, '0.8186')]\n",
      "f1: [0.8186, 0.7659]\n",
      "accuracy: [0.9301, 0.892]\n",
      "Best test f1: 0.8186\n",
      "Best test accuracy: 0.9301\n",
      "Best test recall: 0.9063\n",
      "Best test precision: 0.7692\n",
      "Best AUC: 0.9622\n",
      "====================\n",
      "Epoch 2\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:43<00:00,  1.72it/s]\n",
      "process time: 158.9254031976064\n",
      "loss = 0.0694\n",
      "Macro-accuracy = 0.9723\n",
      "Macro-recall = 0.9723\n",
      "Macro- precision = 0.9723\n",
      "Macro-F1 = 0.9723\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:25<00:00,  4.79it/s]\n",
      "loss = 0.3528\n",
      "Macro-accuracy = 0.9251\n",
      "Macro-recall = 0.9036\n",
      "Macro- precision = 0.7593\n",
      "Macro-F1 = 0.8095\n",
      "AUC = 0.9646\n",
      "Best epoch: [(0, '0.8186')]\n",
      "f1: [0.8186, 0.7659, 0.8095]\n",
      "accuracy: [0.9301, 0.892, 0.9251]\n",
      "Best test f1: 0.8186\n",
      "Best test accuracy: 0.9301\n",
      "Best test recall: 0.9063\n",
      "Best test precision: 0.7692\n",
      "Best AUC: 0.9622\n",
      "====================\n",
      "Epoch 3\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:36<00:00,  1.78it/s]\n",
      "process time: 158.4648243018559\n",
      "loss = 0.0065\n",
      "Macro-accuracy = 0.9871\n",
      "Macro-recall = 0.9871\n",
      "Macro- precision = 0.9872\n",
      "Macro-F1 = 0.9871\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:24<00:00,  4.80it/s]\n",
      "loss = 0.2112\n",
      "Macro-accuracy = 0.9523\n",
      "Macro-recall = 0.8585\n",
      "Macro- precision = 0.8384\n",
      "Macro-F1 = 0.8481\n",
      "AUC = 0.9731\n",
      "Best epoch: [(0, '0.8186'), (3, '0.8343'), (4, '0.8406'), (6, '0.8481')]\n",
      "f1: [0.8186, 0.7659, 0.8095, 0.8343, 0.8406, 0.8362, 0.8481]\n",
      "accuracy: [0.9301, 0.892, 0.9251, 0.9405, 0.9471, 0.9538, 0.9523]\n",
      "Best test f1: 0.8481\n",
      "Best test accuracy: 0.9523\n",
      "Best test recall: 0.8585\n",
      "Best test precision: 0.8384\n",
      "Best AUC: 0.9731\n",
      "====================\n",
      "Epoch 7\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:31<00:00,  1.85it/s]\n",
      "process time: 157.55417856574059\n",
      "loss = 0.0038\n",
      "Macro-accuracy = 0.9926\n",
      "Macro-recall = 0.9926\n",
      "Macro- precision = 0.9926\n",
      "Macro-F1 = 0.9926\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:22<00:00,  5.43it/s]\n",
      "loss = 0.1717\n",
      "Macro-accuracy = 0.9304\n",
      "Macro-recall = 0.9007\n",
      "Macro- precision = 0.7696\n",
      "Macro-F1 = 0.8175\n",
      "AUC = 0.9696\n",
      "Best epoch: [(0, '0.8186'), (3, '0.8343'), (4, '0.8406'), (6, '0.8481')]\n",
      "f1: [0.8186, 0.7659, 0.8095, 0.8343, 0.8406, 0.8362, 0.8481, 0.8175]\n",
      "accuracy: [0.9301, 0.892, 0.9251, 0.9405, 0.9471, 0.9538, 0.9523, 0.9304]\n",
      "Best test f1: 0.8481\n",
      "Best test accuracy: 0.9523\n",
      "Best test recall: 0.8585\n",
      "Best test precision: 0.8384\n",
      "Best AUC: 0.9731\n",
      "====================\n",
      "Epoch 8\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:33<00:00,  1.83it/s]\n",
      "process time: 157.0538592338562\n",
      "loss = 0.0008\n",
      "Macro-accuracy = 0.9944\n",
      "Macro-recall = 0.9944\n",
      "Macro- precision = 0.9944\n",
      "Macro-F1 = 0.9944\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:24<00:00,  4.93it/s]\n",
      "loss = 0.1205\n",
      "Macro-accuracy = 0.9515\n",
      "Macro-recall = 0.8638\n",
      "Macro- precision = 0.8337\n",
      "Macro-F1 = 0.8479\n",
      "AUC = 0.9739\n",
      "Best epoch: [(0, '0.8186'), (3, '0.8343'), (4, '0.8406'), (6, '0.8481')]\n",
      "f1: [0.8186, 0.7659, 0.8095, 0.8343, 0.8406, 0.8362, 0.8481, 0.8175, 0.8479]\n",
      "accuracy: [0.9301, 0.892, 0.9251, 0.9405, 0.9471, 0.9538, 0.9523, 0.9304, 0.9515]\n",
      "Best test f1: 0.8481\n",
      "Best test accuracy: 0.9523\n",
      "Best test recall: 0.8585\n",
      "Best test precision: 0.8384\n",
      "Best AUC: 0.9731\n",
      "====================\n",
      "Epoch 9\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:44<00:00,  1.71it/s]\n",
      "process time: 157.74857850074767\n",
      "loss = 0.0013\n",
      "Macro-accuracy = 0.9946\n",
      "Macro-recall = 0.9946\n",
      "Macro- precision = 0.9946\n",
      "Macro-F1 = 0.9946\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:25<00:00,  4.67it/s]\n",
      "loss = 0.1356\n",
      "Macro-accuracy = 0.9538\n",
      "Macro-recall = 0.8209\n",
      "Macro- precision = 0.8616\n",
      "Macro-F1 = 0.8397\n",
      "AUC = 0.9746\n",
      "Best epoch: [(0, '0.8186'), (3, '0.8343'), (4, '0.8406'), (6, '0.8481')]\n",
      "f1: [0.8186, 0.7659, 0.8095, 0.8343, 0.8406, 0.8362, 0.8481, 0.8175, 0.8479, 0.8397]\n",
      "accuracy: [0.9301, 0.892, 0.9251, 0.9405, 0.9471, 0.9538, 0.9523, 0.9304, 0.9515, 0.9538]\n",
      "Best test f1: 0.8481\n",
      "Best test accuracy: 0.9523\n",
      "Best test recall: 0.8585\n",
      "Best test precision: 0.8384\n",
      "Best AUC: 0.9731\n",
      "====================\n",
      "Epoch 10\n",
      "====================\n",
      "Training: 100%|███████████████████████████████| 280/280 [02:43<00:00,  1.71it/s]\n",
      "process time: 158.30838372490624\n",
      "loss = 0.0001\n",
      "Macro-accuracy = 0.9975\n",
      "Macro-recall = 0.9975\n",
      "Macro- precision = 0.9975\n",
      "Macro-F1 = 0.9975\n",
      "Testing: 100%|████████████████████████████████| 120/120 [00:24<00:00,  4.81it/s]\n",
      "loss = 0.0958\n",
      "Macro-accuracy = 0.9504\n",
      "Macro-recall = 0.8461\n",
      "Macro- precision = 0.8347\n",
      "Macro-F1 = 0.8403\n",
      "AUC = 0.9732\n",
      "Best epoch: [(0, '0.8186'), (3, '0.8343'), (4, '0.8406'), (6, '0.8481')]\n",
      "f1: [0.8186, 0.7659, 0.8095, 0.8343, 0.8406, 0.8362, 0.8481, 0.8175, 0.8479, 0.8397, 0.8403]\n",
      "accuracy: [0.9301, 0.892, 0.9251, 0.9405, 0.9471, 0.9538, 0.9523, 0.9304, 0.9515, 0.9538, 0.9504]\n",
      "Best test f1: 0.8481\n",
      "Best test accuracy: 0.9523\n",
      "Best test recall: 0.8585\n",
      "Best test precision: 0.8384\n",
      "Best AUC: 0.9731\n",
      "====================\n",
      "============early stopping==============\n",
      "now: 10    last: 6\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python train_joint.py \\\n",
    "  -bs=32 \\\n",
    "  -lr_other=5e-5 \\\n",
    "  -lr_gat=1e-2 \\\n",
    "  -ep=20 \\\n",
    "  -dr=0.5 \\\n",
    "  -ad=0.1 \\\n",
    "  -hs=768 \\\n",
    "  --model=bert \\\n",
    "  --clip \\\n",
    "  --cuda=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JWH7MOuyysZs"
   },
   "outputs": [],
   "source": [
    "# !python train_joint.py \\\n",
    "#   -bs=64 \\\n",
    "#   -lr_other=5e-5 \\\n",
    "#   -lr_gat=1e-2 \\\n",
    "#   -ep=20 \\\n",
    "#   -dr=0.5 \\\n",
    "#   -ad=0.1 \\\n",
    "#   -hs=768 \\\n",
    "#   --model=jointv2 \\\n",
    "#   --clip \\\n",
    "#   --cuda=1 "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
